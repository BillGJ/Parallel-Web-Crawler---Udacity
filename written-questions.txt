Written Questions

Q1. Run the web crawler using the configurations located at src/main/config/written_question_1a.json and
    src/main/config/written_question_1b.json. The only difference between these configurations is that one always uses
    the sequential crawler and the other always uses the parallel crawler. Inspect the profile output in
    profileData.txt.

    If you are using a multi-processor computer, you should notice that SequentialWebCrawler#crawl and
    ParallelWebCrawler#crawl took about the same amount of time, but PageParserImpl#parse took much longer when run with
    the ParallelWebCrawler.

    Why did the parser take more time when run with ParallelWebCrawler?

A1. The parse takes more time when run with the ParallelWebCrawler because it parses more web pages in the ParallelWebCrawler
    version than in the SequentialWebCrawler version.
    Here is the comparison:
    
	. Sequential Web Crawler:
	
	com.udacity.webcrawler.SequentialWebCrawler#crawl took 0m 2s 309ms
	com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 290ms

	{"wordCounts":{"learning":109,"machine":82,"udacity":78,"with":65,"program":57},"urlsVisited":4}

	. Parallel Web Crawler:
	
	com.udacity.webcrawler.ParallelWebCrawler#crawl took 0m 2s 738ms
	com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 12s 636ms

	{"wordCounts":{"data":986,"learning":707,"program":622,"udacity":609,"with":600},"urlsVisited":38}


Q2. Your manager ran your crawler on her old personal computer, using the configurations from Q1, and she notices that
    the sequential crawler actually outperforms the parallel crawler. She would like to know why.

    (a) Suggest one reason why the sequential web crawler was able to read more web pages than the parallel crawler.
        (Hint: Try setting "parallelism" to 1 in the JSON configs to simulate your manager's computer.)
	
     A2a. One reasyon why the sequential web crawler was able to read more pages than the parallel crawler is because
	    the code for the parallel crawler is optimized for computers with multiple processors, so when my manager
          runs the code on a old computer and set "parallelism" to 1, the parallel crawer is not able to run on multiple threads
          therefore it is slow and in that scenario slower than the sequential web crawler

    (b) Suggest one scenario in which the parallel web crawler will almost certainly perform better than the sequential
        crawler. Why will it perform better?
     
    A2b. One scenario in which the parallel web crawler will almost certainly perform better than the sequential crawler is when
         "parallelism" is set to a value superior to 1 in both a sequential config file and a parallel config file as in question 1


Q3. Analyze your method profiler through the lens of Aspect Oriented Programming, by answering the following questions:

    (a) What cross-cutting concern is being addressed by the com.udacity.webcrawler.profiler.Profiler class?
	
    
    A3a. The cross-cutting concern being addressed by the com.udacity.webcrawler.profiler.Profiler class is the measure of performance

    
   (b) What are the join points of the Profiler in the web crawler program?
   
   A3b. The join points of the Profiler in the web crawler program are the methods annoted with the @Profiled annotation  


Q4. Identify three (3) different design patterns used in this project, and explain which interfaces, classes, and/or
    libraries use or implement those design patterns.

    For each pattern, name one thing about the pattern that you LIKED, and one thing you DISLIKED. If you did not like
    anything, you can name two things you disliked.

    A4. Three different design patterns used in this project are :
        - The Dependency Injection Pattern. This pattern is used through the "ParallelWebCrawler" and "SequentialWebCrawler" classes,
          that implement "WebCrawler" interface. Guice is used extensively in order to harness that pattern. One thing I like
          about this pattern is that final instances don't need be instantiated when being used and the code is cleaner, one thing I don't
	    like is that forgetting an @Inject annotation causes errors that can be hard to debug.

        - The Dynamic Proxy Pattern. This pattern is used through the profilers code particularly the "ProfilerImpl" and 
          the "ProfilingMethodInterceptor" classes. What I like about this pattern is its underlying level of abstraction
          when a class decorates another class.

        - The Builder Pattern. This pattern is used in the "CrawlerConfiguration" class. What I like about it is that it makes instantiating
          a class with multiples parameters in its constructor easy. You don't need to worry about the order of such parameters, you just to
          remember to call the setters of these parameters when needed.
      

